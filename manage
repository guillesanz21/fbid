#! /usr/bin/env python3

# Primeros pasos:
#     - download
#     - package
#     - train (before running scala container)

import os
import sys
import argparse
import time

def main():
    parser = argparse.ArgumentParser(description="This script deploys a big data infrastructure whose purpose is to predict flight delays.")

    parser.add_argument("--up", help="Deploy the infrastructure. Options: docker, docker-compose|compose")

    parser.add_argument("--down", action="store_true", help="Brings down the services and removes the docker containers and docker images.")

    parser.add_argument("--pull", action="store_true", help="Pull all the needed base images from dockerhub")

    parser.add_argument("--download", action="store_true", help="Download the data (flights).")

    parser.add_argument("--train", action="store_true", help="Train the model with data")

    parser.add_argument("--package", action="store_true", help="Package the scala files")

    parser.add_argument("--airflow", action="store_true", help="Deploys Apache Airflow")
    
    args = parser.parse_args()

    if not args.up and not args.down and not args.train \
        and not args.download and not args.pull and not args.package and not args.airflow:
        parser.parse_args(["--help"])
        sys.exit(0)

    # print(args)

    data = not not (False or os.popen("ls data | grep simple_flight_delay*").read())
    packaged = not not (False or os.popen("ls docker/resources | grep   flight_prediction*.jar").read())
    trained = not not (False or os.popen("ls models | grep string_indexer_model*").read())
    sparkBuilded = not not (False or os.popen("docker images | grep fbid.spark").read())

    try:
        if args.down:
            down()
            return
        if args.download:
            download_data()
            download = True
            return
        if args.pull:
            pullImages()
            return
        if args.train:
            if data and sparkBuilded:
                train()
                trained = True
            return
        if args.package:
            packageScala()
            packaged = True
            return
        if args.up:
            if not pull:
                pullImages()
            if args.up == "docker":
                down()
                deploy_docker()
                return
            elif args.up in ["docker-compose", "compose"]:
                compose_down()
                deploy_compose()
                return
            else:
                print("--up options: [docker, docker-compose|compose]")
                return
        if args.airflow:
            airflow()
        else:
            parser.parse_args([--help])
        sys.exit(0)
    except Exception as e:
        print("Error!")
        print(e)
        sys.exit(0)

def compose_down():
    print("Stoping the containers:")
    c("docker-compose down")
    c("docker-compose rm")
    time.sleep(2)
    c("docker rmi fbid.kafka fbid.zookeeper fbid.zoo fbid.mongo fbid.spark fbid.train fbid.spark-submit fbid.web")
    print("Deleting custom network")
    c("docker network rm fbid_fbid")


def down():
    print("Stopping the containers:")
    c("docker stop kafka zoo mongo spark worker master spark-submit train web airflow")
    time.sleep(2)
    c("docker rm kafka zoo mongo spark worker master spark-submit train web airflow")
    time.sleep(2)
    c("docker rmi fbid.kafka fbid.zookeeper fbid.zoo fbid.mongo fbid.spark fbid.train fbid.spark-submit fbid.web fbid.airflow")


    # print("Deleting the custom network...")
    # c("docker network rm fbid_fbid")

    # TODO: Delete volumes (?)

def deploy_docker(trained=False):
    
    print("\n\n########################")
    print("Deploying with docker...")
    print("########################\n")

    print ("\n########## Creating the custom network...  ##########")
    c("docker network create fbid_fbid")


    print("\n########## Deploying zookeeper... ##########")
    c("docker build . -f docker/Dockerfile.zk -t fbid.zoo \
        && docker run --name zoo -d -p 2181:2181 --network fbid_fbid \
        --hostname zookeeper fbid.zoo")
      

    time.sleep(3)

    print("\n########## Deploying kafka... ##########")
    c("docker build . -f docker/Dockerfile.kafka -t fbid.kafka \
        && docker run --name kafka -d -p 9092:9092 --network fbid_fbid \
        -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 \
        -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 \
        -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
        --hostname kafka fbid.kafka")
    
    # time.sleep(3)

    # c("docker exec kafka /kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list")

    print("\n########## Deploying MongoDB... ##########")
    c("docker build . -f docker/Dockerfile.mongo -t fbid.mongo \
        && docker run --name mongo -d -p 27017:27017 --network fbid_fbid \
        -v ${PWD}/data/mongo:/flights/db \
        -e MONGODB_URI=mongodb://mongo:27017/ \
        --hostname mongo fbid.mongo")


    print("\n########## Deploying Spark base image... ##########")
    c("docker build . -f docker/Dockerfile.spark -t fbid.spark")

    sparkBuilded = True
    if not trained:
        train()
    
    # MASTER
    print("\n########## Deploying SPARK MASTER... ##########")
    c("docker run --name master -d -p 7077:7077 -p 8080:8080 \
        -e SPARK_MODE=master --network fbid_fbid \
        --hostname master bitnami/spark:3.1.2")
    # WORKER
    print("\n########## Deploying SPARK WORKER... ##########")
    c("docker run --name worker -d -p 8081:8081 --network fbid_fbid \
        -e SPARK_MODE=worker \
        -e SPARK_MASTER_URL=spark://master:7077 \
        --hostname worker bitnami/spark:3.1.2")

    # SPARK-SUBMIT
    print("\n########## Deploying SPARK SUBMIT... ##########")
    c("docker build . -f docker/Dockerfile.spark-submit -t fbid.spark-submit \
        && docker run --name spark-submit -d --network fbid_fbid \
        --hostname sparksubmit fbid.spark-submit")

    time.sleep(5)
    print("\n########## Deploying FLASK WEB... ##########")
    c("docker build . -f docker/Dockerfile.web -t fbid.web \
        && docker run --name web -p 5000:5000 -d --network fbid_fbid \
        -e MONGO_URI=mongodb://mongo:27017 fbid.web")

    # airflow()
    

def deploy_compose():
    print("Deploying with docker-compose...")
    c("docker network create fbid_fbid")
    c("docker-compose up -d")
    print("DONE")

def pullImages():
    pull("alpine:3.13.6")
    pull("mongo:5.0.3")
    pull("openjdk:8-jdk")
    pull("hseeberger/scala-sbt:8u312_1.5.5_2.12.15")
    pull("bitnami/spark:3.1.2")

def pull(img):
    c(f"docker pull {img}")

def download_data():
    c("resources/download_data.sh")

def train():
    print("\n########## Deploying training container... ##########")
    c("docker build . -f docker/Dockerfile.spark -t fbid.spark")
    time.sleep(2)
    c("docker build . -f docker/Dockerfile.train -t fbid.train \
        && docker run --name train --network host \
        -v ${PWD}/models:/spark/models fbid.train")
    
    time.sleep(3)

def airflow():
    print("\n########## Deploying airflow container... ##########")
    c("docker build . -f docker/Dockerfile.airflow -t fbid.airflow")
    time.sleep(2)
    c("docker run --name airflow --network host -d -p 8000:8000 fbid.airflow")
    time.sleep(2)

def packageScala():
    c("docker run -it -v ${PWD}/flight_prediction:/flight_prediction \
        --rm hseeberger/scala-sbt:8u312_1.5.5_2.12.15 \
        bash -c 'cd /flight_prediction && sbt package'")
    c("cp flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar docker/resources/.")
    # c("rm -rf flight_prediction/target")

def c(cmd):
    os.system(cmd)

if __name__ == '__main__':
    main()
